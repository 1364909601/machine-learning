假如你想深入机器学习和它背后的数学，你将会很快意识到一切都可归结为一个优化问题。就连训练神经网络都是一个参数优化的问题。因此要想理解机器学习算法，你需要首先理解数学优化的基本概念，以及它为什么这么有用。



这篇文章，你将会看到一步一步的演示如何求解一个简单的机器学习问题。在这个过程中，你将会看到为什么以及它怎样归结为一个机器学习问题，参数如何被优化以及如何计算最优值。



## Step 1: 问题定义



如下所示一个简单的数据集，一共包括11个已知点，点的坐标为： (x1, x2):

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-651w3zlk)

x1,x2的含义可以任意假定，比如x1 表示计算机的使用年限，x2表示训练神经网络所需要的时间。



如果你的电脑使用年限为下图中的x, 显然你不知道训练神经网络所需要的准确时间x2

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-vz6p3z5j)

## Step 2: 仅凭猜测

我们随便找一个电脑使用年限与x 最接近的点，如下图所示，来预测x对应的x2值，产生的误差为err.



![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-ev9r3zgz)

但是，如果我们运气不好，下图所示，已知的11个点中，没有与预测点的使用年限x值相近的，此时误差就会很大。

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-rxf93zo0)

因此，我们需要一个更好的算法来求解。



## Step 3: 让猜测更准确：机器学习方法

## **a. 数据拟合**

现在我们进入机器学习领域。在观察红色数据点后，你会很容易看出一种线性趋势，你的电脑使用越长(x1 越大)，训练时间就会越长(x2 越大)。一个更好的算法将会依据数据，识别出这个趋势，做出更好的预测，误差自然会更小。



![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-7olv3zzm)

函数应该近似拟合我们的数据

## **b. 拟合线**

我们怎样发现这条线？利用高中知识：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-edqf3znz)

我们只需要找到更好的a和b：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-opuy3zar)

对于此处的数据，最优值分别为 **a=0.8** and **b=20**. 但是，我们应该怎么发现这些最优值 **a** and **b**

## **c. 发现最优a和b，最小化均方误差**

Well, as we said earlier, we want to find **a** and **b** such that the line **y=ax+b** fits our data as good as possible. Or, mathematically speaking, the error / distance between the points in our dataset and the line should be minimal.

就像我们前面说的，我们想要发现**a** 和 **b**，以便使得 **y=ax+b**这条线尽可能好的拟合我们的数据。用数学的术语来表述，在我们数据集中的点与这条线的误差(或称距离)应该是最小的，如下图所示，这些黄色的虚线尽可能地短。



![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-y0wu3znk)

绿色箭头指向的点，其误差(或称距离)距离等于：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-3o11z3zqf)

如果此点的坐标为**(x1, x2) = (100, 120)**， 还记得最优参数**a=0.8** and **b=20**，所以此点的误差：

**Error = f(x) — yi**
Error = f(100) — 120
Error = a\*100+b — 120
Error = 0.8\*100+20–120
**Error = -12**

图中我们也可看出这个点比拟合线高12个单元。为了评估拟合线整体的表现，需要计算所有点的误差。怎么做？首先计算每个点的误差的平方。有两个原因：

1. 通过平方误差，我们会得到一个绝对值，(-12)^2 = 144, 
2. 平方放大了误差，如果某些点离拟合线比较远，结果误差将会变得非常大

用公式表达为：

Then, let’s sum up the errors to get an estimate of the overall error:

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-lb13m3z84)

更一般的写法：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-h815h3zlz)

这个公式称为均方误差的和(sum of square error)，它在统计学和机器学习中非常常见。



## **d. 优化函数**

优化函数为什么这么有用？初始问题的定义：我们想要发现a和b以便 **y=ax+b**更好的拟合数据集。也可以表述为，我们想要发现 **a** 和**b**以便均方误差最小，即：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-tk17a3zax)

If we find the minimum of this function **f(a, b)**, we have found our optimal a and b values:

假如找到函数 **f(a, b)**的最小值，我们将会发现a和b的最优值：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-w71au3z1c)

Before we get into actual calculations, let’s give a graphical impression of how our optimization function **f(a, b)** looks like:

进入实际计算前，先让我们图形绘制下优化函数 **f(a, b)**：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-5g1ci3za2)

左图的高度代表均方误差的大小，山峰越高，误差越大。最小均方误差位于绿色箭头所指处，此处对应的a和b就是最优值。



沿着轴a，改变a的取值(对应左图的斜率变大)，相应的误差也会变大。

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-xg1du3zo7)

 沿着轴b，改变b的取值，也就是叫线上下移动，我们同样也会得到更大的均方误差。

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-kw1fc3zrl)

## **e. 计算最优值**

怎样计算均方误差最小值对应的a和b呢？ 我们知道取得全局最小点必须满足两个条件：

1. 一阶偏导  **f’(a,b) = 0** 
2. 二阶偏导 **f’’(a,b) >0** 



因为目标函数是2个维度的函数，我们可以简单的计算每一个维度上的偏导数，根据必须满足的条件1，得到如下方程：

**f(a, b) Δa = 0**

**f(a, b) Δb = 0**

 已知 **f(a,b) = SUM [axi+b — yi]²** ，进一步化简：

 **f(a,b) = SUM [yi² + b²+a²x + 2abxi — 2byi — 2bxiyi]**， 综合可得：

**SUM [yi² + b²+a²xi + 2abxi — 2byi — 2axiyi] Δa = 0**

**SUM [yi² + b²+a²xi + 2abxi — 2byi — 2axiyi] Δb = 0**

很容易计算出偏导数：

**f(a,b) = SUM [2axi + 2bxi — 2xiyi] = 0**

**f(a,b) = SUM [2b+ 2axi — 2yi ] = 0**

记住，在上面两个方程中，有两个SUM，和许多已知点xi 和yi.  就算只有10个点，代入后方程也会变得很长，因此手算也变得不容易，只能借助计算机。



恭喜你！你已经知道线性回归是如何工作的，以及如何通过已知样本学习到两个参数a,b.

但是，要等一下，还有更多知识需要了解。

## Approximating Polynomials

如果我们的数据没有展现线性走势，而是像如下一个二次函数走势：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-8l1gs3z31)

在这种情况下，很明显线性函数拟合的将不会太好，多项式将会更好，这就意味着我们需要处理平方函数，三次方函数，甚至更高阶的多项式逼近。



但是，计算这些问题的方法通用的，首先我们再次定义问题，我们想要一个平方函数 **y = ax² + bx + c** 以此更好的拟合我们的数据：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-nr1nk3z7c)



就像你看到的，我们现在有三个参数要去确定：**a, b** 和**c**.  因为，我们的最小化问题需要稍作改变，不过均方误差和最小的目标函数不会改变：

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-p71ow3zao)

需要优化的三个参数 **a,b** 和**c**:

![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-9a1qk3z1o)



## 过拟合

我们仍然有一个问题需要去解释，文章一开始提到的具有线性走势的那个问题，难道就不能用二次函数拟合吗？肯定可以！我们用更高阶的函数**y = ax² + bx + c** 去拟合不好吗？如果单从训练集上的表现来看，高阶函数肯定会让误差更小的。

事实上，目标函数的阶次比总的样本点个数小于1的时候，我们的目标函数将会拟合到每一个点，均方误差将会是**0**. 非常完美，不是吗？



并不是那样好，用下面两幅图来解释：



![img](https://hackernoon.com/photos/QNBTOZ8CHSUXADIOeFQWR1lzsxA2-4z1sb3zqm)

在左图，我们用二次函数近似拟合数据，在右图，我们用10阶函数拟合，因此它能近似拟合几乎所有14个样本点。



但是，右图的线条运动看起来很奇怪，它试图去拟合所有点，但是它丢失了样本数据的整体走势。得到的高阶函数对值得变化更加敏感，预测结果更加不可靠。



因此，我们应该首先观察数据，决策用几阶多项式拟合效果很最好，然后选择合适的阶数去拟合。